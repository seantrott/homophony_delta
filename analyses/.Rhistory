theme_bw()
s = transit_cost %>%
mutate(tunnel_per2 = tunnel / length) %>%
# filter(country %in% c("US", "UK", "TR")) %>%
group_by(country) %>%
summarise(mean_cost = mean(cost_km_millions),
mean_tunnel = mean(tunnel_per2))
s$mean_tunnel
hist(s$mean_tunnel)
transit_cost %>%
mutate(tunnel_per2 = tunnel / length) %>%
# filter(country %in% c("US", "UK", "TR")) %>%
group_by(country) %>%
summarise(mean_cost = mean(cost_km_millions),
mean_tunnel = mean(tunnel_per2)) %>%
ggplot(aes(x = reorder(country, mean_cost),
y = mean_cost,
fill = mean_tunnel)) +
geom_hline(yintercept = mean(transit_cost$cost_km_millions),
linetype = "dotted") +
geom_bar(stat = "identity") +
scale_fill_viridis_b() +
theme_bw()
s = transit_cost %>%
mutate(tunnel_per2 = tunnel / length) %>%
# filter(country %in% c("US", "UK", "TR")) %>%
group_by(country) %>%
summarise(mean_cost = mean(cost_km_millions),
mean_tunnel = mean(tunnel_per2))
View(s)
transit_cost <- tuesdata$transit_cost
transit_cost %>%
mutate(tunnel_per2 = tunnel / length) %>%
# filter(country %in% c("US", "UK", "TR")) %>%
group_by(country) %>%
summarise(mean_cost = mean(cost_km_millions),
mean_tunnel = mean(tunnel_per2)) %>%
ggplot(aes(x = reorder(country, mean_cost),
y = mean_cost,
fill = mean_tunnel)) +
geom_hline(yintercept = mean(transit_cost$cost_km_millions),
linetype = "dotted") +
geom_bar(stat = "identity") +
scale_fill_viridis_b() +
theme_bw()
s = transit_cost %>%
mutate(tunnel_per2 = tunnel / length) %>%
# filter(country %in% c("US", "UK", "TR")) %>%
group_by(country) %>%
summarise(mean_cost = mean(cost_km_millions),
mean_tunnel = mean(tunnel_per2))
View(s)
View(transit_cost)
s = c(0.001245, 0.1874)
p.adjust(s, "holm")
-1 ** log10(.05)
-1 * log10(.05)
vector
s = list(c( 1, 2, 3))
s
Unlist(s)
unlist(s)
a = c(3, 0, TRUE)
b = c(4, 0, FALSE)
c = (a | b)
c
x = as.Date("2018-10-01")
x
month(x)
attr(x)
as.month(x)
months(x)
as.POSIXct("Sep-28-2019 7:54:31 AM")
as.POSIXct("Sep-28-2019 7:54:31 AM UTC", format = '%b-%d-%Y')
as.POSIXct("Sep-28-2019 7:54:31 AM", format = '%b-%d-%Y')
format(as.POSIXct("Sep-28-2019 7:54:31 AM", format = '%b-%d-%Y'))
library(DescTools)
v <- 1:3
names(v) <- c("a", "b", "c")
v[4] < -4
v[4] <- 4
names(v[4])
which
7L
type(7L)
typeof(7L)
```{r include=FALSE}
library(tidyverse)
library(forcats)
library(broom)
read_data = function(agg_path) {
# Read in aggregated data
df_agg = read_csv(agg_path)
# Below, we identify the number of *words* of each word length.
df_total_counts = df_agg %>%
mutate(k_actual = num_homophones + 1) %>%
group_by(num_sylls_est) %>%
summarise(M = sum(k_actual))
# Merge with counts
df_agg = merge(df_agg, df_total_counts, by = "num_sylls_est")
nrow(df_agg)
df_agg
}
```
We then define a function to **normalize** the probability of a given wordform relative to the sum of probabilities of wordforms for that length.
```{r}
normalize_probabilities = function(df) {
### For a given dataframe, normalize probabilities "p" relative to #words of a given elgnth.
## First get probabilities
df = df %>%
mutate(normalized_surprisal = heldout_surprisal/num_phones,
p = 10 ** heldout_log_prob)
# Calculate sum(p) for each syllable length bucket.
df_syl_sums = df %>%
group_by(num_sylls_est) %>%
summarise(total_prob = sum(p))
# Merge with aggregated data, and normalize
df = df %>%
left_join(df_syl_sums, on = "num_sylls_est") %>%
mutate(p_normalized = p / total_prob)
df
}
```
We now define a function to compute the expected number of homophones $k_i$ for a given wordform $w_i$. This will also calculate the `delta` between the real and expected amount.
```{r}
compute_expected_homophones = function(df) {
## Compute expected homophones "k" of a wordform by multiplying normalized probability
## "p" by the number of meanings "M".
df = df %>%
# Expected number of "successes" (entries), minus 1
mutate(k = p_normalized * M - 1) %>%
mutate(delta = num_homophones - k)
df
}
```
We also define a function to run the main regression:
```{r}
run_regression = function(df) {
# Initialize output
out = list()
# Build full model
model_full = lm(data = df,
delta ~ frequency + num_sylls_est + normalized_surprisal)
# Build reduced model
model_reduced = lm(data = df,
delta ~ num_sylls_est + normalized_surprisal)
# Run model comparison
comparison = anova(model_reduced, model_full)
df_comp = broom::tidy(comparison) %>%
na.omit()
# Tidy model output
df_model = broom::tidy(model_full)
# Add to list parameters
out$model = df_model
out$comparison = df_comp
out
}
```
And several plotting functions:
```{r}
plot_outcome = function(df_output) {
df_output$model$predictor = fct_recode(
df_output$model$term,
"Number of Syllables" = "num_sylls_est",
"Normalized Surprisal" = "normalized_surprisal",
"Log(Frequency)" = "frequency"
# "Neighborhood Size" = "neighborhood_size"
)
### Plots outcome of regression
g = df_output$model %>%
ggplot(aes(x = predictor,
y = estimate)) +
geom_point() +
coord_flip() +
geom_hline(yintercept = 0, linetype = "dotted") +
geom_errorbar(aes(ymin = estimate - 2*std.error,
ymax = estimate + 2*std.error),
width=.2,
position=position_dodge(.9)) +
labs(x = "Predictor",
y = "Estimate") +
theme_minimal()
g
}
plot_comparison = function(df) {
# Plots expected vs. actual per each wordform.
g = df %>%
ggplot(aes(x = k,
y = num_homophones)) +
geom_point(alpha = .5) +
scale_x_continuous(limits = c(-1, max(df$k))) +
scale_y_continuous(limits = c(0, max(df$k))) +
geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
labs(x = "Expected number of homophones",
y = "Actual number of homophones") +
theme_minimal()
g
}
plot_bins = function(df, n, column, label) {
# Plots delta ~ frequency bins.
df$binned = as.numeric(ntile(pull(df, column), n = n))
g = df %>%
group_by(binned) %>%
summarise(mean_delta = mean(delta),
se_delta = sd(delta) / sqrt(n())) %>%
ggplot(aes(x = binned,
y = mean_delta)) +
geom_point(stat = "identity", size = .2) +
geom_hline(yintercept = 0, linetype = "dotted") +
geom_errorbar(aes(ymin = mean_delta - se_delta,
ymax = mean_delta + se_delta),
width=.2,
position=position_dodge(.9)) +
labs(x = label,
y = "Delta (Real - Expected)") +
geom_smooth() +
theme_minimal()
g
}
plot_contrast = function(df, bins = 20) {
df$frequency_binned = as.numeric(ntile(df$frequency, n = bins))
g = df %>%
mutate(expected = k,
actual = num_homophones) %>%
pivot_longer(c(expected, actual), names_to = "model", values_to = "homophones") %>%
ggplot(aes(x = frequency_binned,
y = homophones,
color = model)) +
stat_summary (fun = function(x){mean(x)},
fun.min = function(x){mean(x) - sd(x)/sqrt(length(x))},
fun.max = function(x){mean(x) + sd(x)/sqrt(length(x))},
geom= 'pointrange') +
# position=position_dodge(width=0.95)) +
labs(x = "Binned Frequency",
y = "Number of Homophones") +
theme_bw()
g
}
setwd("/Users/seantrott/Dropbox/UCSD/Research/Ambiguity/Evolution/homophony_delta/analyses")
df_english = read_data("../data/processed/english/reals/english_with_mps_5phone_holdout.csv")
nrow(df_english)
## Now normalize probabilities and compute expected homophones per wordform
df_english = df_english %>%
normalize_probabilities() %>%
compute_expected_homophones()
## Double-check that this equals correct amount in lexicon
df_english %>%
group_by(num_sylls_est) %>%
summarise(total = sum(k + 1),
correct_total = mean(M))
```
We then merge with data for frequency of each individual lemma. (We also calculate entropy over senses in the process.)
```{r}
df_english_lemmas = read_delim("../data/frequency/english/lemmas.csv", delim = "\\",
quote = "")
nrow(df_english_lemmas)
df_english_lemmas = df_english_lemmas %>%
mutate(freq_adjusted = Cob + 1) %>%
group_by(PhonDISC) %>%
mutate(total_frequency = sum(freq_adjusted),
relative_frequency = freq_adjusted / total_frequency)
df_entropy = df_english_lemmas %>%
group_by(PhonDISC) %>%
summarise(entropy = -sum(relative_frequency * log(relative_frequency)),
total_frequency = mean(total_frequency))
df_merged_english = df_english %>%
# inner_join(df_entropy, by = "PhonDISC")
inner_join(df_entropy, by = "PhonDISC")
nrow(df_merged_english)
df_merged_english$frequency = log10(df_merged_english$total_frequency)
```
Now we can visualize the outcome in a variety of ways:
```{r}
df_merged_english %>%
plot_comparison()
df_merged_english %>%
plot_bins(n = 20, column = "frequency", label = "Binned frequency")
```
And finally, we can run the regression and visualize model coefficients:
```{r}
output = df_merged_english %>%
run_regression()
output %>%
plot_outcome()
output$model
output$comparison
```
Directly contrast the relationships:
```{r}
df_merged_english %>%
plot_contrast(bins = 20)
```
### Calculate Sense Entropy
```{r}
nrow(filter(df_merged_english, num_homophones == 1))
model_full = lm(data = filter(df_merged_english, num_homophones == 1),
delta ~ entropy + frequency + num_sylls_est + normalized_surprisal)
summary(model_full)
model_reduced = lm(data = filter(df_merged_english, num_homophones ==1),
delta ~ frequency + num_sylls_est + normalized_surprisal)
anova(model_reduced, model_full)
```
## Dutch
First, we load and process the data.
```{r}
df_dutch = read_data("../data/processed/dutch/reals/dutch_with_mps_5phone_holdout.csv")
nrow(df_dutch)
## Now normalize probabilities and compute expected homophones per wordform
df_dutch = df_dutch %>%
normalize_probabilities() %>%
compute_expected_homophones()
## Double-check that this equals correct amount in lexicon
df_dutch %>%
group_by(num_sylls_est) %>%
summarise(total = sum(k + 1),
correct_total = mean(M))
```
We then merge with data for frequency of each individual lemma. (We also calculate entropy over senses in the process.)
```{r}
df_dutch_lemmas = read_delim("../data/frequency/dutch/lemmas.csv", delim = "\\")
nrow(df_dutch_lemmas)
df_dutch_lemmas = df_dutch_lemmas %>%
mutate(freq_adjusted = Inl + 1) %>%
group_by(PhonDISC) %>%
mutate(total_frequency = sum(freq_adjusted),
relative_frequency = freq_adjusted / total_frequency)
df_entropy = df_dutch_lemmas %>%
group_by(PhonDISC) %>%
summarise(entropy = -sum(relative_frequency * log(relative_frequency)),
total_frequency = mean(total_frequency))
df_merged_dutch = df_dutch %>%
# inner_join(df_entropy, by = "PhonDISC")
inner_join(df_entropy, by = "PhonDISC")
nrow(df_merged_dutch)
df_merged_dutch$frequency = log10(df_merged_dutch$total_frequency)
```
Now we can visualize the outcome in a variety of ways:
```{r}
df_merged_dutch %>%
plot_comparison()
df_merged_dutch %>%
plot_bins(n = 20, column = "frequency", label = "Binned frequency")
```
And finally, we can run the regression and visualize model coefficients:
```{r}
output = df_merged_dutch %>%
run_regression()
output %>%
plot_outcome()
output$model
output$comparison
```
Directly contrast the relationships:
```{r}
df_merged_dutch %>%
plot_contrast(bins = 20)
```
### Calculate Sense Entropy
```{r}
nrow(filter(df_merged_dutch, num_homophones == 1))
model_full = lm(data = filter(df_merged_dutch, num_homophones == 1),
delta ~ entropy + frequency + num_sylls_est + normalized_surprisal)
summary(model_full)
model_reduced = lm(data = filter(df_merged_dutch, num_homophones ==1),
delta ~ frequency + num_sylls_est + normalized_surprisal)
anova(model_reduced, model_full)
```
## German
```{r}
df_german = read_data("../data/processed/german/reals/german_with_mps_5phone_holdout.csv")
nrow(df_german)
## Now normalize probabilities and compute expected homophones per wordform
df_german = df_german %>%
normalize_probabilities() %>%
compute_expected_homophones()
## Double-check that this equals correct amount in lexicon
df_german %>%
group_by(num_sylls_est) %>%
summarise(total = sum(k + 1),
correct_total = mean(M))
```
We then merge with data for frequency of each individual lemma. (We also calculate entropy over senses in the process.)
```{r}
df_german_lemmas = read_delim("../data/frequency/german/lemmas.csv", delim = "\\")
nrow(df_german_lemmas)
df_german_lemmas = df_german_lemmas %>%
mutate(freq_adjusted = Mann + 1) %>%
group_by(PhonDISC) %>%
mutate(total_frequency = sum(freq_adjusted),
relative_frequency = freq_adjusted / total_frequency)
df_entropy = df_german_lemmas %>%
group_by(PhonDISC) %>%
summarise(entropy = -sum(relative_frequency * log(relative_frequency)),
total_frequency = mean(total_frequency))
df_merged_german = df_german %>%
# inner_join(df_entropy, by = "PhonDISC")
inner_join(df_entropy, by = "PhonDISC")
nrow(df_merged_german)
df_merged_german$frequency = log10(df_merged_german$total_frequency)
```
Now we can visualize the outcome in a variety of ways:
```{r}
df_merged_german %>%
plot_comparison()
df_merged_german %>%
plot_bins(n = 20, column = "frequency", label = "Binned frequency")
```
And finally, we can run the regression and visualize model coefficients:
```{r}
output = df_merged_german %>%
run_regression()
output %>%
plot_outcome()
output$model
output$comparison
```
Directly contrast the relationships:
```{r}
df_merged_german %>%
plot_contrast(bins = 20)
```
### Calculate Sense Entropy
```{r}
nrow(filter(df_merged_german, num_homophones == 1))
model_full = lm(data = filter(df_merged_german, num_homophones == 1),
delta ~ entropy + frequency + num_sylls_est + normalized_surprisal)
summary(model_full)
model_reduced = lm(data = filter(df_merged_german, num_homophones ==1),
delta ~ frequency + num_sylls_est + normalized_surprisal)
anova(model_reduced, model_full)
```
## French
```{r}
df_french = read_data("../data/processed/french/reals/french_with_mps_4phone_holdout.csv")
nrow(df_french)
## Now normalize probabilities and compute expected homophones per wordform
df_french = df_french %>%
normalize_probabilities() %>%
compute_expected_homophones()
## Double-check that this equals correct amount in lexicon
df_french %>%
group_by(num_sylls_est) %>%
summarise(total = sum(k + 1),
correct_total = mean(M))
```
Read in French lemmas.
```{r}
df_french_lemmas = read_csv("../data/processed/french/reals/french_all_reals_4phone.csv")
nrow(df_french_lemmas)
sum(df_french_lemmas$`8_freqlemlivres`)
df_french_lemmas = df_french_lemmas %>%
## Multiply by 14.8, b/c measures were divided by 14.8 in Lexique
mutate(freq_adjusted = `8_freqlemlivres` * 14.8) %>%
group_by(`2_phon`) %>%
summarise(total_frequency = sum(freq_adjusted))
nrow(df_french_lemmas)
sum(df_french_lemmas$total_frequency)
df_french_merged = df_french %>%
inner_join(df_french_lemmas, by = "2_phon")
nrow(df_french)
nrow(df_french_merged)
df_french_merged$frequency = log10(df_french_merged$total_frequency + 1)
```
And finally, we can run the regression and visualize model coefficients:
```{r}
output = df_french_merged %>%
run_regression()
output$model
output$comparison
```
Directly contrast the relationships:
```{r}
df_french_merged %>%
plot_contrast(bins = 20)
df_french_lemmas = read_csv("../data/processed/french/reals/french_all_reals_4phone.csv")
df_french_lemmas$freq_adjusted = df_french_lemmas$`10_freqlemlivres` * 14.8 + 1
nrow(df_french_lemmas)
df_french_lemmas = df_french_lemmas %>%
# mutate(freq_adjusted = log10(frequency + 1) + .01) %>%
group_by(`2_phon`) %>%
mutate(total_frequency = sum(freq_adjusted),
relative_frequency = freq_adjusted / total_frequency)
df_entropy = df_french_lemmas %>%
group_by(`2_phon`) %>%
summarise(entropy = -sum(relative_frequency * log(relative_frequency)))
df_french_entropy = df_french_merged %>%
inner_join(df_entropy, by = "2_phon")
nrow(df_french_entropy)
nrow(df_french_merged)
nrow(filter(df_french_entropy, num_homophones == 1))
df_french_lemmas = read_csv("../data/processed/french/reals/french_all_reals_4phone.csv")
df_french_lemmas$freq_adjusted = df_french_lemmas$`8_freqlemlivres` * 14.8 + 1
nrow(df_french_lemmas)
df_french_lemmas = df_french_lemmas %>%
# mutate(freq_adjusted = log10(frequency + 1) + .01) %>%
group_by(`2_phon`) %>%
mutate(total_frequency = sum(freq_adjusted),
relative_frequency = freq_adjusted / total_frequency)
df_entropy = df_french_lemmas %>%
group_by(`2_phon`) %>%
summarise(entropy = -sum(relative_frequency * log(relative_frequency)))
df_french_entropy = df_french_merged %>%
inner_join(df_entropy, by = "2_phon")
nrow(df_french_entropy)
nrow(df_french_merged)
nrow(filter(df_french_entropy, num_homophones == 1))
model_full = lm(data = filter(df_french_entropy, num_homophones == 1),
delta ~ entropy + frequency + num_sylls_est + normalized_surprisal)
summary(model_full)
model_reduced = lm(data = filter(df_french_entropy, num_homophones ==1),
delta ~ frequency + num_sylls_est + normalized_surprisal)
anova(model_reduced, model_full)
